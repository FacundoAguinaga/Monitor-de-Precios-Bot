# üöÄ Market Intelligence & Price Monitor Hub

![Python](https://img.shields.io/badge/Python-3.13-blue?style=for-the-badge&logo=python&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
![Playwright](https://img.shields.io/badge/Playwright-Automation-green?style=for-the-badge&logo=google-chrome&logoColor=white)
![Tests](https://img.shields.io/badge/Tests-Pytest-yellow?style=for-the-badge&logo=pytest&logoColor=white)

> **Plataforma profesional de Business Intelligence para monitoreo competitivo de precios en e-commerce**

Sistema automatizado que integra descubrimiento de competidores, scraping as√≠ncrono de alto rendimiento y dashboards interactivos para la toma de decisiones basada en datos.

## üìã Tabla de Contenidos

- [Caracter√≠sticas](#-caracter√≠sticas-principales)
- [Arquitectura](#-arquitectura-t√©cnica)
- [Instalaci√≥n](#-instalaci√≥n)
- [Uso](#-gu√≠a-de-uso)
- [Configuraci√≥n](#-configuraci√≥n-avanzada)
- [Tests](#-testing)
- [Limitaciones](#-limitaciones-conocidas)
- [Roadmap](#-roadmap)

## üåü Caracter√≠sticas Principales

### 1. üïµÔ∏è Auto-Descubrimiento Inteligente
- **B√∫squeda automatizada** basada en keywords con filtrado inteligente
- **Modo Reemplazo/Acumulativo** para gesti√≥n flexible de objetivos
- **Detecci√≥n anti-publicidad** para URLs limpias y v√°lidas
- Manejo robusto de layouts Grid/List de MercadoLibre

### 2. ü§ñ Motor ETL de Alto Rendimiento
| Tecnolog√≠a | Prop√≥sito |
|------------|-----------|
| **Playwright** | Navegaci√≥n headless con JavaScript rendering |
| **Asyncio** | Operaciones as√≠ncronas para velocidad 10x |
| **Stealth Mode** | Rotaci√≥n de User-Agents + anti-detecci√≥n |
| **Auto-Retry** | Reintentos exponenciales ante fallos de red |

### 3. üìä Dashboard Anal√≠tico Interactivo
- **KPIs en Tiempo Real**: Precio promedio, m√≠nimo, volatilidad
- **Visualizaciones Plotly**: Gr√°ficos de tendencias y comparativas
- **CRUD Completo**: Gesti√≥n de base de datos desde UI
- **Sistema de Logs**: Auditor√≠a completa de operaciones

## üèóÔ∏è Arquitectura T√©cnica

```mermaid
graph TD
    A[Usuario] -->|Busca productos| B[Discovery Module]
    B -->|Guarda URLs| C[(products.csv)]
    C -->|Lee objetivos| D[Scraper Engine]
    D -->|Extrae datos| E[Playwright Browser]
    E -->|HTML parsing| F[Data Processor]
    F -->|Almacena| G[(history.csv)]
    F -->|Sync| H[Google Sheets API]
    G -->|Lee| I[Streamlit Dashboard]
    I -->|Visualiza| A
    
    style D fill:#4CAF50
    style I fill:#FF4B4B
    style E fill:#40B5AD
```

### Estructura del Proyecto

```text
Monitor-de-Precios-Bot/
‚îú‚îÄ‚îÄ üìÇ data/                    # Persistencia de datos
‚îÇ   ‚îú‚îÄ‚îÄ history.csv             # Historial de precios scrapeados
‚îÇ   ‚îî‚îÄ‚îÄ precios_hoy.csv         # Snapshot del d√≠a (legacy)
‚îú‚îÄ‚îÄ üìÇ src/                     # M√≥dulos core del sistema
‚îÇ   ‚îú‚îÄ‚îÄ discover.py             # Crawler de b√∫squeda autom√°tica
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py              # Motor de extracci√≥n (ETL)
‚îÇ   ‚îú‚îÄ‚îÄ sheet_manager.py        # Integraci√≥n Google Sheets API
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # [NUEVO] Configuraci√≥n centralizada
‚îÇ   ‚îî‚îÄ‚îÄ main.py                 # Orquestador de backend
‚îú‚îÄ‚îÄ üìÇ tests/                   # [NUEVO] Suite de testing
‚îÇ   ‚îî‚îÄ‚îÄ test_scraper.py         # Tests unitarios cr√≠ticos
‚îú‚îÄ‚îÄ üìÑ app.py                   # Frontend Streamlit
‚îú‚îÄ‚îÄ üìÑ products.csv             # Base de datos de objetivos
‚îú‚îÄ‚îÄ üìÑ bot_activity.log         # Logs de auditor√≠a
‚îú‚îÄ‚îÄ üìÑ config.yaml              # [NUEVO] Configuraci√≥n externa
‚îú‚îÄ‚îÄ üìÑ requirements.txt         # Dependencias Python
‚îú‚îÄ‚îÄ üìÑ .env.example             # [NUEVO] Template de variables
‚îî‚îÄ‚îÄ üìÑ README.md                # Esta documentaci√≥n
```

## ‚öôÔ∏è Instalaci√≥n

### Prerequisitos
- Python 3.11+ (recomendado 3.13)
- Git
- Navegador Chromium (instalado autom√°ticamente por Playwright)

### Instalaci√≥n Paso a Paso

```bash
# 1. Clonar repositorio
git clone https://github.com/FacundoAguinaga/Monitor-de-Precios-Bot.git
cd Monitor-de-Precios-Bot

# 2. Crear entorno virtual
python -m venv venv

# Activar en Linux/Mac
source venv/bin/activate

# Activar en Windows
venv\Scripts\activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Instalar navegadores de Playwright
playwright install chromium

# 5. Configurar variables de entorno (opcional)
cp .env.example .env
# Editar .env con tus credenciales de Google Sheets
```

### Configuraci√≥n Inicial (Opcional - Google Sheets)

```bash
# 1. Crear proyecto en Google Cloud Console
# 2. Habilitar Google Sheets API
# 3. Descargar service_account.json
# 4. Colocar en la ra√≠z del proyecto
```

## üéØ Gu√≠a de Uso

### Inicio R√°pido (3 minutos)

```bash
# Iniciar dashboard
streamlit run app.py
```

El sistema abrir√° autom√°ticamente en `http://localhost:8501`

### Flujo de Trabajo Recomendado

```mermaid
sequenceDiagram
    participant U as Usuario
    participant D as Discovery Tab
    participant G as Gesti√≥n Tab
    participant E as Ejecuci√≥n Tab
    participant Dash as Dashboard Tab
    
    U->>D: 1. Buscar "iPhone 15"
    D->>G: Guardar 10 URLs
    U->>G: 2. Revisar y filtrar
    U->>E: 3. Ejecutar scraping
    E->>Dash: Guardar en history.csv
    U->>Dash: 4. Analizar resultados
```

### Modos de Operaci√≥n

**Modo Discovery (Exploraci√≥n):**
```python
# Desde la UI:
1. Tab "Discovery" ‚Üí Ingresar keyword: "Notebook Gamer"
2. Cantidad: 10
3. Toggle "Modo Reemplazo" OFF (acumulativo)
4. Click "Buscar y Procesar"
```

**Modo Manual (Scraping On-Demand):**
```python
# Desde la UI:
1. Tab "Ejecuci√≥n" ‚Üí Click "EJECUTAR SCRAPING"
2. Esperar progreso (barra de carga)
3. Ver resultados en Dashboard
```

**Modo CLI (Avanzado):**
```bash
# Ejecutar desde terminal
python src/main.py
```

## üîß Configuraci√≥n Avanzada

### config.yaml (Personalizaci√≥n)

```yaml
scraper:
  timeout: 30000
  headless: true
  max_retries: 3
  delay_between_requests: 2  # segundos

discovery:
  default_limit: 5
  max_results: 20

storage:
  history_file: "data/history.csv"
  targets_file: "products.csv"
```

### Variables de Entorno (.env)

```bash
GOOGLE_SHEETS_NAME=Precios_Competencia
SERVICE_ACCOUNT_PATH=service_account.json
LOG_LEVEL=INFO
```

## üß™ Testing

```bash
# Instalar dependencias de testing
pip install pytest pytest-asyncio pytest-cov

# Ejecutar tests
pytest tests/

# Con reporte de cobertura
pytest --cov=src tests/

# Tests espec√≠ficos
pytest tests/test_scraper.py -v
```

### Ejemplo de Test

```python
# tests/test_scraper.py
def test_clean_url():
    """Verifica limpieza de URLs con par√°metros"""
    from src.scraper import ProductScraper
    scraper = ProductScraper()
    dirty_url = "https://example.com?param=1#anchor"
    # Implementaci√≥n pendiente de m√©todo p√∫blico
```

## ‚ö†Ô∏è Limitaciones Conocidas

| Limitaci√≥n | Impacto | Soluci√≥n Futura |
|------------|---------|-----------------|
| **CSV como DB** | No escalable | Migrar a PostgreSQL/MongoDB |
| **Sin rate limiting** | Riesgo de ban | Implementar backoff exponencial |
| **Selectores hardcoded** | Fr√°gil ante cambios | Config YAML con fallbacks |
| **Sin autenticaci√≥n** | Productos premium inaccesibles | Integrar Playwright con sesiones |
| **Sincr√≥nico en UI** | Bloquea interfaz | Background tasks con Celery |

### Consideraciones √âticas/Legales

‚öñÔ∏è **DISCLAIMER**: Este proyecto fue desarrollado con **fines exclusivamente educativos** para demostrar:
- T√©cnicas de web scraping moderno
- Arquitectura de sistemas ETL
- Desarrollo de dashboards anal√≠ticos

**NO est√° dise√±ado para:**
- Uso comercial sin autorizaci√≥n
- Scraping masivo que impacte servidores
- Violaci√≥n de Terms of Service

**Recomendaci√≥n**: Siempre revisa los `robots.txt` y TOS del sitio objetivo. Considera usar APIs oficiales cuando est√©n disponibles.

## üöß Roadmap

### v2.0 (Pr√≥xima Release)
- [x] Sistema de logging profesional
- [x] Dashboard interactivo
- [ ] **Tests con >70% coverage**
- [ ] **Configuraci√≥n externa completa**
- [ ] **Dockerizaci√≥n** (docker-compose listo para deploy)
- [ ] **Type hints** completos en todo el codebase

### v2.5 (Futuro)
- [ ] Base de datos PostgreSQL/MongoDB
- [ ] Sistema de alertas (Telegram/Discord/Email)
- [ ] Scheduler autom√°tico (Cron/APScheduler)
- [ ] Soporte multi-sitio (Amazon, Falabella, etc.)
- [ ] API REST para integraci√≥n con otros sistemas

### v3.0 (Visi√≥n)
- [ ] Machine Learning para predicci√≥n de precios
- [ ] Dashboard en tiempo real (WebSockets)
- [ ] Autenticaci√≥n de usuarios (multi-tenant)
- [ ] Deploy en cloud (AWS/GCP/Azure)

## üìà M√©tricas del Proyecto

```text
L√≠neas de C√≥digo:     ~800 (Python)
Cobertura de Tests:   [Pendiente]
Tiempo de Scraping:   ~2s por producto
Uptime Promedio:      N/A (uso manual)
```

## ü§ù Contribuciones

Este es un proyecto de portafolio personal, pero acepto sugerencias:

1. Fork el proyecto
2. Crea una feature branch (`git checkout -b feature/MejoraSugerida`)
3. Commit tus cambios (`git commit -m 'Agrega funcionalidad X'`)
4. Push a la branch (`git push origin feature/MejoraSugerida`)
5. Abre un Pull Request

## üìù Licencia

Este proyecto est√° bajo licencia MIT - ver archivo `LICENSE` para detalles.

## üë§ Autor

**Facundo Aguinaga**
- GitHub: [@FacundoAguinaga](https://github.com/FacundoAguinaga)
- LinkedIn: [https://www.linkedin.com/in/facundo-aguinaga-707b01356/]
- Email: [aguinagafacuno8@gmail.com]

## üôè Agradecimientos

- Streamlit por el framework de dashboards
- Playwright por la librer√≠a de automatizaci√≥n
- MercadoLibre (como caso de estudio educativo)

---

**‚ö° Pro Tip**: Para mejores resultados, ejecuta el scraping en horarios de bajo tr√°fico (madrugada) para reducir latencia y probabilidad de bloqueos.

**üêõ Encontraste un bug?** Abre un issue con:
- Descripci√≥n del problema
- Pasos para reproducir
- Screenshot/logs relevantes

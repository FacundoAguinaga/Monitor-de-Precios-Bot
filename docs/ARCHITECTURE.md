# üèóÔ∏è Arquitectura del Sistema

> **Documento T√©cnico**: Detalles de implementaci√≥n, decisiones de dise√±o y patrones arquitect√≥nicos.

## üìä Visi√≥n General

El **Price Monitor Hub** es una aplicaci√≥n de Business Intelligence dise√±ada con arquitectura modular de 3 capas:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         CAPA DE PRESENTACI√ìN            ‚îÇ
‚îÇ  (Streamlit Dashboard - app.py)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        CAPA DE L√ìGICA DE NEGOCIO        ‚îÇ
‚îÇ  (Scrapers, Discovery, Processors)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       CAPA DE PERSISTENCIA              ‚îÇ
‚îÇ  (CSV Files, Google Sheets API)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîß Componentes Principales

### 1. Discovery Module (`src/discover.py`)

**Responsabilidad**: B√∫squeda autom√°tica de productos en e-commerce.

**Tecnolog√≠as**:
- Playwright (navegaci√≥n headless)
- Asyncio (operaciones as√≠ncronas)

**Flujo**:
```
Usuario ingresa keyword
    ‚Üì
B√∫squeda en MercadoLibre
    ‚Üì
Filtrado de URLs v√°lidas
    ‚Üì
Almacenamiento en products.csv
```

**Decisiones de Dise√±o**:
- **Anti-detecci√≥n**: Rotaci√≥n de User-Agents
- **Selectores flexibles**: Soporta Grid y List layout
- **Modo Reemplazo/Acumulativo**: Flexibilidad en gesti√≥n de objetivos

### 2. Scraper Engine (`src/scraper.py`)

**Responsabilidad**: Extracci√≥n de datos de productos.

**Patrones Implementados**:

#### Retry Pattern (Resiliencia)
```python
async def scrape_with_retry(url, max_retries=3):
    for attempt in range(max_retries):
        try:
            return await scrape(url)
        except TimeoutError:
            wait = 2 ** attempt  # Backoff exponencial
            await asyncio.sleep(wait)
    return None
```

#### Fallback Pattern (Robustez)
```python
PRICE_SELECTORS = [
    ".ui-pdp-price__second-line .andes-money-amount__fraction",
    ".price-tag-fraction",  # Fallback 1
    ".andes-money-amount__fraction"  # Fallback 2
]

for selector in PRICE_SELECTORS:
    element = await page.query_selector(selector)
    if element:
        return await element.inner_text()
```

**Optimizaciones**:
- **Browser Reuse**: Una instancia de browser para m√∫ltiples scraping
- **Async/Await**: Hasta 10x m√°s r√°pido vs sincr√≥nico
- **Stealth Mode**: Evasi√≥n de detecci√≥n de bots

### 3. Configuration Module (`src/config.py`)

**Responsabilidad**: Centralizaci√≥n de configuraci√≥n.

**Principio**: **DRY (Don't Repeat Yourself)**

```python
# ‚ùå ANTES (hardcoded en m√∫ltiples archivos)
timeout = 30000
headless = True
selectors = [".price", ".andes-price"]

# ‚úÖ DESPU√âS (centralizado)
from config import config
config.scraper.timeout
config.scraper.headless
config.selectors.PRICE_SELECTORS
```

**Ventajas**:
- Cambios en un solo lugar
- F√°cil testing (mock config)
- Type safety con dataclasses

### 4. Dashboard (`app.py`)

**Arquitectura**: **MVC Pattern adaptado a Streamlit**

```
DataManager (Model)
    ‚Üì
UIComponents (View)
    ‚Üì
ScraperController (Controller)
```

**Separaci√≥n de Responsabilidades**:

| Clase | Responsabilidad |
|-------|----------------|
| `DataManager` | CRUD de datos (CSV) |
| `UIComponents` | Renderizado visual |
| `ScraperController` | Orquestaci√≥n de scraping |

## üóÑÔ∏è Modelo de Datos

### Entities

#### Product Target
```python
{
    "url": str  # URL √∫nica del producto
}
```

#### Price Record
```python
{
    "producto": str,     # Nombre del producto
    "precio": int,       # Precio sin decimales
    "url": str,          # URL fuente
    "moneda": str,       # C√≥digo ISO (ARS, USD)
    "fecha": datetime    # Timestamp de captura
}
```

### Storage Strategy

**Elecci√≥n actual**: CSV Files

**Justificaci√≥n**:
- ‚úÖ Simple y portable
- ‚úÖ No requiere servidor de BD
- ‚úÖ F√°cil debugging (human-readable)
- ‚ùå No escalable (>10k registros)
- ‚ùå Sin transacciones ACID

**Migraci√≥n futura**: PostgreSQL/MongoDB

```sql
-- Schema propuesto para PostgreSQL
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    url VARCHAR(500) UNIQUE NOT NULL,
    name VARCHAR(200),
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE price_history (
    id SERIAL PRIMARY KEY,
    product_id INTEGER REFERENCES products(id),
    price INTEGER NOT NULL,
    currency CHAR(3),
    scraped_at TIMESTAMP DEFAULT NOW(),
    INDEX idx_product_date (product_id, scraped_at)
);
```

## üîÑ Flujos de Datos

### Flujo 1: Discovery ‚Üí Scraping ‚Üí Dashboard

```mermaid
sequenceDiagram
    participant U as Usuario
    participant D as Discovery
    participant CSV as products.csv
    participant S as Scraper
    participant H as history.csv
    participant Dash as Dashboard
    
    U->>D: Buscar "iPhone 15"
    D->>CSV: Guardar 10 URLs
    U->>S: Ejecutar scraping
    S->>CSV: Leer URLs
    loop Para cada URL
        S->>S: scrape_mercadolibre()
    end
    S->>H: Append resultados
    U->>Dash: Ver Dashboard
    Dash->>H: Leer datos
    Dash->>U: Mostrar gr√°ficos
```

### Flujo 2: Error Handling

```mermaid
graph TD
    A[Inicio Scraping] --> B{Request exitoso?}
    B -->|S√≠| C[Parsear HTML]
    B -->|No| D{Reintentos < 3?}
    D -->|S√≠| E[Wait exponencial]
    E --> A
    D -->|No| F[Log error]
    F --> G[Retornar None]
    
    C --> H{Precio encontrado?}
    H -->|S√≠| I[Retornar data]
    H -->|No| J[Probar fallback]
    J --> H
```

## üöÄ Optimizaciones de Performance

### 1. Async Concurrency

**Problema**: Scraping secuencial es lento (2s √ó 10 URLs = 20s)

**Soluci√≥n**:
```python
async def scrape_multiple(urls, max_concurrent=3):
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def bounded_scrape(url):
        async with semaphore:  # L√≠mite de concurrencia
            return await scrape(url)
    
    tasks = [bounded_scrape(url) for url in urls]
    return await asyncio.gather(*tasks)
```

**Resultado**: 10 URLs en ~6-8 segundos (3x concurrente)

### 2. Browser Reuse

**Problema**: Lanzar browser por cada scraping (overhead ~2s)

**Soluci√≥n**: Singleton pattern
```python
class ProductScraper:
    _browser = None
    
    async def _get_browser(self):
        if self._browser is None:
            self._browser = await playwright.chromium.launch()
        return self._browser
```

**Ahorro**: ~2s √ó (n-1) URLs

## üîê Consideraciones de Seguridad

### 1. Anti-Bot Detection

**T√©cnicas implementadas**:
- User-Agent rotation
- Random delays
- JavaScript execution support (Playwright)
- No automation flags

```python
await page.add_init_script("""
    Object.defineProperty(navigator, 'webdriver', {
        get: () => undefined  # Oculta flag de automatizaci√≥n
    });
""")
```

### 2. Rate Limiting (Pendiente)

**Implementaci√≥n propuesta**:
```python
from datetime import datetime, timedelta
from collections import deque

class RateLimiter:
    def __init__(self, max_requests=10, window=60):
        self.requests = deque()
        self.max_requests = max_requests
        self.window = timedelta(seconds=window)
    
    async def acquire(self):
        now = datetime.now()
        # Limpiar requests antiguos
        while self.requests and self.requests[0] < now - self.window:
            self.requests.popleft()
        
        if len(self.requests) >= self.max_requests:
            wait = (self.requests[0] + self.window - now).total_seconds()
            await asyncio.sleep(wait)
        
        self.requests.append(now)
```

## üìà Escalabilidad

### Limitaciones Actuales

| M√©trica | L√≠mite Actual | Causa |
|---------|---------------|-------|
| URLs simult√°neas | ~50 | RAM browser instances |
| Registros hist√≥ricos | ~50k | CSV parsing time |
| Usuarios concurrentes | 1 | Streamlit single-user |

### Propuesta de Arquitectura Escalable

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Load Balancer ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ                ‚îÇ                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Streamlit 1  ‚îÇ ‚îÇ Streamlit 2 ‚îÇ ‚îÇ Streamlit N ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                ‚îÇ                ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Message Queue ‚îÇ
                    ‚îÇ   (Redis/RabbitMQ)‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ                ‚îÇ                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Worker 1    ‚îÇ ‚îÇ  Worker 2   ‚îÇ ‚îÇ  Worker N   ‚îÇ
    ‚îÇ  (Scraping)  ‚îÇ ‚îÇ  (Scraping) ‚îÇ ‚îÇ  (Scraping) ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                ‚îÇ                ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   PostgreSQL    ‚îÇ
                    ‚îÇ   (Time-series) ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üß™ Testing Strategy

### Pir√°mide de Tests

```
        /\
       /  \      E2E Tests (5%)
      /‚îÄ‚îÄ‚îÄ‚îÄ\     - Flow completo Discovery‚ÜíScraping‚ÜíDashboard
     /  ‚ö†Ô∏è  \    
    /‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\   Integration Tests (15%)
   /  üîó  üîó  \  - Scraper + Real MercadoLibre
  /‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ - CSV persistence
 / ‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°‚ö° \ Unit Tests (80%)
/‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ - URL cleaning, price parsing
```

### Coverage Goals

| M√≥dulo | Target Coverage |
|--------|----------------|
| `scraper.py` | 90% |
| `discover.py` | 85% |
| `config.py` | 100% |
| `app.py` | 60% (UI dif√≠cil de testear) |

## üìö Decisiones de Dise√±o

### ¬øPor qu√© Playwright y no Selenium?

| Aspecto | Playwright | Selenium |
|---------|-----------|----------|
| Velocidad | ‚ö°‚ö°‚ö° | ‚ö°‚ö° |
| API Async | ‚úÖ | ‚ùå |
| Wait autom√°tico | ‚úÖ | Parcial |
| Dev Experience | Excelente | Buena |

### ¬øPor qu√© Streamlit y no Flask/Django?

**Decisi√≥n**: Streamlit para MVP r√°pido

**Trade-offs**:
- ‚úÖ Desarrollo 10x m√°s r√°pido
- ‚úÖ UI reactiva autom√°tica
- ‚úÖ Ideal para Data Apps
- ‚ùå Limitado multi-user
- ‚ùå Menos control sobre routing

## üîÆ Roadmap T√©cnico

### v2.0 - Mejoras de Calidad
- [ ] Test coverage >80%
- [ ] Type checking con mypy
- [ ] Linting con flake8/black
- [ ] Pre-commit hooks

### v2.5 - Escalabilidad
- [ ] Migraci√≥n a PostgreSQL
- [ ] Background workers (Celery)
- [ ] Redis para caching
- [ ] API REST (FastAPI)

### v3.0 - Features Avanzadas
- [ ] Machine Learning (predicci√≥n de precios)
- [ ] WebSockets (real-time updates)
- [ ] Multi-tenancy (usuarios/empresas)
- [ ] Alertas autom√°ticas

---

**√öltima actualizaci√≥n**: Enero 2026  
**Autor**: Facundo Aguinaga